{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3149b5ce",
   "metadata": {},
   "source": [
    "## Paper Explanation\n",
    "#### Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb2d20",
   "metadata": {},
   "source": [
    "#### Background\n",
    "<font size=2>\n",
    "    \n",
    "Multivariate time series forecasting sometimes have both short- and long-term periodical pattern, which is not well predicted by traditional methods, like **Autoregression** and **Gaussion Process**. A new approach with **neural network** is necessary to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df707b",
   "metadata": {},
   "source": [
    "#### Core innovation: recurrsive-skip\n",
    "<font size=2>\n",
    "    \n",
    "Besides the normal RNN component to predict time-sequence-related data, they propose a new method to extract multi-periodical behavior. For example, in case of traffic, the occupancy rate of a road has both periodicities in one single day and in one week, which exhibits high occupancy during moring- and evening-peak in workdays and low occupancy during weekends.\n",
    "\n",
    "In order to track the pattern of similar periodicity, they also add a component focusing on some specific time epoch. For instance, to predict occupancy of road at 7:00 to find out the rule said above, i.e. high during workdays and low during weekends, they set a skip-parameter to only focus on situation at 7:00 every day, which denotes the number of hidden cells skipped through.\n",
    "    \n",
    "<div>\n",
    "<img src=\"paper_explanation_1.png\" style=\"zoom:40%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b212909",
   "metadata": {},
   "source": [
    "#### How they implement\n",
    "<font size=2>\n",
    "   \n",
    "Long- and Short-term Time-series network(LSTNet), which consists of:\n",
    "\n",
    "1. CNN component:\n",
    "    \n",
    "    to extract short-term patterns in the time-sequence and variables;\n",
    "    \n",
    "2. RNN component:\n",
    "    \n",
    "    main part in this network;\n",
    "    \n",
    "3. Recurrsive-skip part:\n",
    "    \n",
    "    the innovatively proposed structure;\n",
    "    to be against gradient vanishment from GRU/RNN in long-term prediction;\n",
    "    \n",
    "4. Autoregressive component:\n",
    "    \n",
    "    drawback of NN: scale of outputs is not sensitive to the scale of input;\n",
    "    focus on the local scaling issue;\n",
    "    \n",
    "5. prediction\n",
    "    \n",
    "    deccompose final prediction into liear and non-liear parts;\n",
    "    linear: autoregressive;\n",
    "    non-linear: RNN output;\n",
    "    \n",
    "6. Frobenius norm or L1Loss as error function\n",
    "    \n",
    "to extract shot- and long-term repetitive behavior in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12861a1b",
   "metadata": {},
   "source": [
    "#### How they evaluate\n",
    "<font size=2>\n",
    "\n",
    "1. Metrics\n",
    "\n",
    "    Root Relative Squared Error (RSE): the lower, the better;\n",
    "\n",
    "    Empirical Correlation Coefficient (CORR): the higher, the better;\n",
    "    \n",
    "2. Comparison group\n",
    "    \n",
    "    introduce a case(exchange-rate) which is not suitable with LSTNet to represent the specific usage of LSTNet;\n",
    "    process identical data with other different models to support their proposed one;\n",
    "    \n",
    "2. Ablation Study\n",
    "    \n",
    "    run the process without specific part to see the effect, to induce the positive influence of it;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92892563",
   "metadata": {},
   "source": [
    "#### Results and Conclusion\n",
    "<font size=2>\n",
    "\n",
    "1.\n",
    "LSTNet outperforms other mothods especially with the large horizons, in capturing both short- and long-term repeating patterns in data;\n",
    "\n",
    "2.\n",
    "The NN component in LSTNet may not be sufficiently sensitive to violated scale fluctuations in data, while the linear AR part can;\n",
    "\n",
    "3.\n",
    "Problem:\n",
    "\n",
    "How to automatically choose **skip-length**?\n",
    "    \n",
    "How to integrate different dimension of variables during CNN part, which in real world usually have different attributes but have been taken equally for now in paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdda220",
   "metadata": {},
   "source": [
    "#### Personal opinions on this paper\n",
    "<font size=2>\n",
    "    \n",
    "1.\n",
    "They emphasize not only the advantages of LSTNet, but also give example(exchange-rate case) which is not feasible to use LSTNet, in other words, the limit of their innovation;\n",
    "\n",
    "2.\n",
    "Ablation study is clear and necessary to show the influence of different part in network, although it can only indicate qualitative, rather than quantitative;\n",
    "\n",
    "3.\n",
    "The innovative **recurrsive-skip** has also a limit, that the hyper-parameter which is called **skip-length** is empirical. And it needs pre-knowledge of dataset, for example, autocorrelation has been checked previously to confirm whether the data has multi-periodical pattern. Even if it is confirmed, then how large the **skip-length** should be is also an empirical choice which even needs iterative operation to find the suitable one. Of course, if it is already clear that what this data is about, like traffic case in paper, which obviously has realtion between workdays and work-hours, it will be easier to confirm **skip-length**. In conclusion, this component is not as general as network structures like CNN, RNN;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a61859",
   "metadata": {},
   "source": [
    "#### SOTA of benchmarks\n",
    "<font size=2>\n",
    "    \n",
    "1. AR: Autoregressive\n",
    "    \n",
    "    predict future values based on past values;\n",
    "    Autoregressive models implicitly assume that the future will resemble the past;\n",
    "    \n",
    "2. LRidge: vector autoregression (VAR) model with L2-regularization\n",
    "    \n",
    "    VAR: relate current observations of a variable with past observations of itself and past observations of other variables in the system;\n",
    "    L2: give penalty when weights are too many to describe the network and cause overfitting;\n",
    "    \n",
    "3. LSVR: VAR model with Support Vector Regression(SVR) objective function\n",
    "    \n",
    "    SVR: has similar concept with SVM but is a regression model. It tries to find the best fitting line within a threshold, which is distance between hyperplane and boundries. Hyperplane is defined by support vectors, which closest distribute along either side of hyperplane and define a margin/ distance. SVR also uses kernel function to transfer data into higher dimension where a hyperplane can be found, e.g. RBF(radical basis function) like gaussian.\n",
    "    \n",
    "4. TRMF: AR model using temporal regularized matrix factorization\n",
    "    \n",
    "5. GP: Gaussian Process for time series modeling\n",
    "    \n",
    "6. VAR-MLP: Multilayer Perception (MLP) with AR\n",
    "    \n",
    "    MLP: a perception usually with one input layer and one outputlayer and multiple hidden layers. Each layer is also with activation function to decide the output of hidden layer. Nodes in each layer are fully connected with the ones of previous layer.\n",
    "    \n",
    "7. RNN-GRU: RNN model using GRU cell.\n",
    "    \n",
    "    RNN: efficient for cases depending on time sequence\n",
    "    GRU: more expressive inner structure to reduce gradient vanishing of RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabeda8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
